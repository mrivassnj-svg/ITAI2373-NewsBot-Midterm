# NewsBot Intelligence System - Midterm Project

## Table of Contents
1. [Introduction](#Introduction)
2. [Dataset Overview](#Dataset-Overview)
3. [Data Preprocessing](#Data-Preprocessing)
4. [Text Classification](#Text-Classification)
5. [Evaluation](#Evaluation)
6. [Conclusion](#Conclusion)

---

## Introduction
In this project, we will develop an NLP-based classification system to categorize news articles into predefined categories.

---

## Dataset Overview
I am are using the **BBC News dataset** consisting of labeled news articles. The dataset is split into training and test sets.

```python
import pandas as pd

# Load the prepared datasets
df_train = pd.read_csv('newsbot_train.csv')
df_test = pd.read_csv('newsbot_test.csv')

# Quick check
print(df_train.head())Data Preprocessing

We clean the text data by removing URLs, punctuation, and extra spaces. The text is then converted to lowercase, and stopwords are removed.
print(df_test.head())

Data Preprocessing

We clean the text data by removing URLs, punctuation, and extra spaces. The text is then converted to lowercase, and stopwords are removed.
import re

# Define a function to clean text
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# Apply to train and test datasets
df_train['content_clean'] = df_train['Text'].apply(clean_text)
df_test['content_clean'] = df_test['Text'].apply(clean_text)

# Display cleaned content
print(df_train['content_clean'].head())
####Text Classification
####
####Here, we will use TF-IDF for text vectorization and Naive Bayes for classification.
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# Vectorization
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
X_train = tfidf.fit_transform(df_train['content_clean'])
X_test = tfidf.transform(df_test['content_clean'])

# Training Naive Bayes Classifier
y_train = df_train['Category']
model = MultinomialNB()
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)
####
####Evaluation
####We evaluate the model's performance using the classification report and confusion matrix.
####
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Classification report
print(classification_report(y_test_true, predictions))

# Confusion matrix
cm = confusion_matrix(y_test_true, predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=df_train['Category'].unique(), yticklabels=df_train['Category'].unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

###Conclusion
###This notebook demonstrated an end-to-end pipeline for classifying news articles. Future work could involve exploring more advanced models, hyperparameter tuning, and real-time article classification.

---

### **`src/`**

#### **`preprocessing.py`**

This Python script handles all text preprocessing functions, such as cleaning, tokenization, and stopword removal.

```python
import re

def clean_text(text):
    text = text.lower()  # lowercase
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # remove URLs
    text = re.sub(r'\W', ' ', text)  # remove punctuation
    text = re.sub(r'\s+', ' ', text)  # remove extra whitespace
    return text.strip()
